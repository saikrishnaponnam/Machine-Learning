
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://saikrishnaponnam.github.io/Machine-Learning/Initializers/">
      
      
        <link rel="prev" href="../math/probability/">
      
      
        <link rel="next" href="../optimizer/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Weight Initializers - Machine Learning Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#weight-initializers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Machine Learning Notes" class="md-header__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Weight Initializers
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/saikrishnaponnam/Machine-Learning.git" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Machine Learning Notes" class="md-nav__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saikrishnaponnam/Machine-Learning.git" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../math/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Fundamentals
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Fundamentals
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../math/calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Calculus
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../math/linear-algebra/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../math/probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Weight Initializers
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Weight Initializers
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#constant-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Constant Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Random Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#xavier-glorot-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Xavier (Glorot) Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kaiming-he-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Kaiming (He) Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#qa" class="md-nav__link">
    <span class="md-ellipsis">
      Q&amp;A
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../optimizer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimizers
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../regularization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Regularization
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../convolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convolution
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../max_pool/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Max Pooling
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../batch_norm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Batch Norm
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#constant-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Constant Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Random Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#xavier-glorot-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Xavier (Glorot) Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kaiming-he-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Kaiming (He) Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#qa" class="md-nav__link">
    <span class="md-ellipsis">
      Q&amp;A
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="weight-initializers">Weight Initializers</h1>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Initialization</th>
<th>Recommended For</th>
<th style="text-align: center;">Preserves Variance</th>
<th>Activation Function</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><code>Zero</code></td>
<td>bias</td>
<td style="text-align: center;"></td>
<td>-</td>
</tr>
<tr>
<td style="text-align: center;"><code>Random</code></td>
<td>Small NN</td>
<td class="red-cross" style="text-align: center;"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg></span></td>
<td>All</td>
</tr>
<tr>
<td style="text-align: center;"><code>Xavier</code></td>
<td>Shallow / Mid-depth</td>
<td class="green-check" style="text-align: center;"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 7 9 19l-5.5-5.5 1.41-1.41L9 16.17 19.59 5.59z"/></svg></span></td>
<td>Tanh, Sigmoid</td>
</tr>
<tr>
<td style="text-align: center;"><code>Kaiming</code></td>
<td>Deep NN</td>
<td class="green-check" style="text-align: center;"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 7 9 19l-5.5-5.5 1.41-1.41L9 16.17 19.59 5.59z"/></svg></span></td>
<td>ReLU, Leaky ReLU</td>
</tr>
</tbody>
</table>
<p>Behaviour of diferent initializers with respect to the variance of activations and gradients can be seen <a href="https://github.com/saikrishnaponnam/Machine-Learning/blob/main/notebooks/initializers.ipynb">here</a></p>
<h2 id="constant-initialization">Constant Initialization</h2>
<p>Weights of a layer are initialized to a fixed constant value. 
Initializing all weights in a layer to a fixed constant value is not recommended. 
When this happens, each neuron in the layer receives the same gradient during backpropagation, causing them to learn identical features. 
This leads to redundancy, as the neurons fail to capture diverse representations. 
Consequently, the model’s ability to learn complex patterns is significantly reduced.</p>
<div class="arithmatex">\[ \mathbf{W}_{ij} = c \quad \forall ~ i,j \]</div>
<h2 id="random-initialization">Random Initialization</h2>
<p>Weights of a layer are initialized to random values. 
Unlike constant initialization, this randomness enables each neuron to learn distinct features.
The weights are usually drawn from a uniform or normal distribution, such as:
$$ \mathbf{W}_{ij} \sim \mathcal{N}(\mu, \sigma^2) $$
While this approach works reasonably well for shallow networks, it can lead to issues like vanishing or exploding gradients in deeper architectures.
This occurs because the variance of activations and gradients may increase or decrease exponentially with the number of layers, primarily due to not accounting for the number of input and output neurons in the initialization scheme.</p>
<h2 id="xavier-glorot-initialization">Xavier (Glorot) Initialization</h2>
<p><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Paper</a><br />
Xavier initialization aims to keep the variance of the activations and gradients constant across layers.  </p>
<p><strong>Why Is It Needed?</strong><br />
If weights are not initialized properly: </p>
<ul>
<li>Too small: Activations and gradients shrink ⇒ vanishing gradients </li>
<li>Too large: Activations and gradients blow up ⇒ exploding gradients  </li>
</ul>
<p><img alt="img_1.png" src="../images/random_init.png" /></p>
<p>Xavier initialization solves this by scaling weights based on the number of input and output units in the layer.
It assumes the activation functions are linear, sigmoid, or tanh.<br />
Let:</p>
<ul>
<li><span class="arithmatex">\(n_{in}\)</span> : number of input units to a layer</li>
<li><span class="arithmatex">\(n_{out}\)</span> : number of output units from a layer</li>
</ul>
<div class="arithmatex">\[ W \sim \mathcal{N}(0, \frac{2}{n_{in} + n_{out}}) \]</div>
<p><strong>Goal of Xavier Initialization</strong><br />
Assume weights <span class="arithmatex">\(w_i\)</span> and <span class="arithmatex">\(x_i\)</span> are independent and identically distributed (i.i.d.) random variables.  </p>
<p>We want to keep the variance of the activations and gradients constant across layers. So:
$$  \text{Var}(a^l) \approx  \text{Var}(a^{l+1}) $$
$$ \text{Var}(\delta^{l}) \approx \text{Var}(\delta^{l+1}) $$
Tanh is linear around origin =&gt; <span class="arithmatex">\(tanh(z) \approx z\)</span> when z is small.</p>
<div class="arithmatex">\[\begin{align*}
 Var(a^{l+1}) &amp;= Var(z^{l+1}) \\
 &amp; = Var(\sum_{i=1}^{n_l} w_i^{l+1}a_i^l ) \\
 &amp;= \sum_{i=1}^{n_l} Var(w_i^{l+1}a_i^l )  \\
 &amp;= \sum_{i=1}^{n_l} Var(w_i^{l+1}) Var(a_i^l) \\ 
 &amp;= n_{l} . Var(w_i^{l+1}) Var(a^l) \\
 Var(W) &amp;= \frac{1}{n_l}  
\end{align*}\]</div>
<p>Similarly for gradient <span class="arithmatex">\(Var(W) = \frac{1}{n_{l+1}}\)</span></p>
<p>To balance the variance of the activations and gradients, we set: $ Var(W) = \frac{2}{n_{in} + n_{out}} $</p>
<h2 id="kaiming-he-initialization">Kaiming (He) Initialization</h2>
<p><a href="https://arxiv.org/abs/1502.01852">Paper</a><br />
<strong>Why Is It Needed?</strong><br />
Xavier initialization assumes that the activation function is either linear, sigmoid, or tanh. However, ReLU is not linear(near origin).
ReLU zeros out all negative values. On average, ReLU passes only about half of its inputs forward, effectively reducing the output variance by approximately 50%. 
This reduction in variance can cause activations to shrink across layers, potentially leading to stalled training. 
In such cases, Xavier initialization underestimates the necessary variance.</p>
<p>Kaiming initialization is designed specifically for ReLU-like activations.</p>
<p><img alt="img.png" src="../images/xavier_relu.png" /></p>
<p>$$ W \sim \mathcal{N}(0, \frac{2}{n_{in}}) $$
The factor 2 compensates for the ReLU “drop” of half the activations</p>
<p><strong>Goal of Xavier Initialization</strong><br />
We want to keep the variance of the forward activations constant across layers. So:
$$  \text{Var}(a^l) \approx  \text{Var}(a^{l-1}) $$</p>
<div class="arithmatex">\[ \begin{align*}
Var(z^{l}) &amp;= Var(\sum_{i=1}^{n_{in}} w_i^{l}a_i^{l-1} ) \\
&amp; = \sum_{i=1}^{n_{in}} Var(w_i^{l}a_i^{l-1})  \\
&amp;= n_{in}  Var(w_i^{l}a_i^{l-1})  \\
&amp;= n_{in}(Var(w_i^{l}) Var(a_i^{l-1}) + \mathbb{E}[w_i^l]^2Var(a_i^{l-1}) + \mathbb{E}[a_i^{l-1}]^2 Var(w_i^l))\\
&amp;= n_{in} (Var(w_i^l) (Var(a_i)^{l-1} + E[a_i^{l-1}]^2)) \\
&amp;= n_{in} (Var(w_i^l) \mathbb{E}[(a_i^{l-1})^2])
\end{align*}\]</div>
<div class="arithmatex">\[ \begin{align*}
\mathbb{E}[(z_i^{l-1})^2]) &amp;= \int_{-\infty}^{\infty} (a_i^{l-1})^2 P_a(a_i^{l-1}) da_i^{l-1} \\
&amp;= \int_{-\infty}^{\infty} max(0, z_i^{l-1})^2 P_a(z_i^{l-1}) dz_i^{l-1} \\
&amp;= \int_{0}^{\infty} (z_i^{l-1})^2 P_z(z_i^{l-1}) dz_i^{l-1}  \\
&amp;= \frac{1}{2} \int_{-\infty}^{\infty} (z_i^{l-1})^2 P_z(z_i^{l-1}) dz_i^{l-1} \\
&amp;= \frac{1}{2} Var(z_i^{l-1})\\
\end{align*}\]</div>
<p>$$ Var(W) = \frac{2}{n_{in}} $$
Empirically, the backward gradients will also behave reasonably as long as the forward activations are well-scaled. So unlike xavier we only try to maintain variance of activations.</p>
<!--## Orthogonal initialization -->

<h2 id="qa">Q&amp;A</h2>
<ol>
<li>
<p>Why is weight initialization important in neural networks?</p>
<ul>
<li>Weight initialization is important because it can significantly affect the convergence speed and performance of the model. Proper initialization helps to maintain a good flow of gradients during training, preventing issues like vanishing or exploding gradients.</li>
</ul>
</li>
<li>
<p>Can biases be initialized to zero? Why or why not?</p>
<ul>
<li>Yes, biases can be initialized to zero. Unlike weights, initializing biases to zero does not cause symmetry issues because biases are added to the output of neurons and do not affect the learning of different features. In fact, initializing biases to zero is a common practice.</li>
</ul>
</li>
<li>
<p>How does weight initialization affect vanishing or exploding gradients?</p>
<ul>
<li>Weight initialization affects vanishing or exploding gradients by influencing the scale of the activations and gradients as they propagate through the layers. If weights are initialized too small, the gradients can vanish, leading to slow convergence. If weights are initialized too large, the gradients can explode, causing instability during training.</li>
</ul>
</li>
<li>
<p>Diagnose an issue in training that could be due to bad initialization.</p>
<ul>
<li>If the model is not learning at all or learning very slowly, it could be due to bad initialization. 
  Visualizing the gradients during training can help identify this issue.</li>
</ul>
</li>
<li>
<p>If your deep model is not learning, what role might initialization play and how would you investigate it?</p>
<ul>
<li>If a deep model isn’t learning, I would suspect initialization if:<ul>
<li>The loss remains constant or decreases very slowly. </li>
<li>Activations or gradients vanish or explode. </li>
<li>All neurons appear to behave identically (symmetry).</li>
</ul>
</li>
<li>Visualizing gradients and activations across layers.</li>
</ul>
</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["content.code.copy"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>