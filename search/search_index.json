{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Title"},{"location":"Initializers/","title":"Weight Initializers","text":""},{"location":"Initializers/#summary","title":"Summary","text":"Initialization Recommended For Preserves Variance Activation Function <code>Zero</code> bias - <code>Random</code> Small NN All <code>Xavier</code> Shallow / Mid-depth Tanh, Sigmoid <code>Kaiming</code> Deep NN ReLU, Leaky ReLU <p>Behaviour of diferent initializers with respect to the variance of activations and gradients can be seen here</p>"},{"location":"Initializers/#constant-initialization","title":"Constant Initialization","text":"<p>Weights of a layer are initialized to a fixed constant value.  Initializing all weights in a layer to a fixed constant value is not recommended.  When this happens, each neuron in the layer receives the same gradient during backpropagation, causing them to learn identical features.  This leads to redundancy, as the neurons fail to capture diverse representations.  Consequently, the model\u2019s ability to learn complex patterns is significantly reduced.</p> \\[ \\mathbf{W}_{ij} = c \\quad \\forall ~ i,j \\]"},{"location":"Initializers/#random-initialization","title":"Random Initialization","text":"<p>Weights of a layer are initialized to random values.  Unlike constant initialization, this randomness enables each neuron to learn distinct features. The weights are usually drawn from a uniform or normal distribution, such as: $$ \\mathbf{W}_{ij} \\sim \\mathcal{N}(\\mu, \\sigma^2) $$ While this approach works reasonably well for shallow networks, it can lead to issues like vanishing or exploding gradients in deeper architectures. This occurs because the variance of activations and gradients may increase or decrease exponentially with the number of layers, primarily due to not accounting for the number of input and output neurons in the initialization scheme.</p>"},{"location":"Initializers/#xavier-glorot-initialization","title":"Xavier (Glorot) Initialization","text":"<p>Paper Xavier initialization aims to keep the variance of the activations and gradients constant across layers.  </p> <p>Why Is It Needed? If weights are not initialized properly: </p> <ul> <li>Too small: Activations and gradients shrink \u21d2 vanishing gradients </li> <li>Too large: Activations and gradients blow up \u21d2 exploding gradients  </li> </ul> <p></p> <p>Xavier initialization solves this by scaling weights based on the number of input and output units in the layer. It assumes the activation functions are linear, sigmoid, or tanh. Let:</p> <ul> <li>\\(n_{in}\\) : number of input units to a layer</li> <li>\\(n_{out}\\) : number of output units from a layer</li> </ul> \\[ W \\sim \\mathcal{N}(0, \\frac{2}{n_{in} + n_{out}}) \\] <p>Goal of Xavier Initialization Assume weights \\(w_i\\) and \\(x_i\\) are independent and identically distributed (i.i.d.) random variables.  </p> <p>We want to keep the variance of the activations and gradients constant across layers. So: $$  \\text{Var}(a^l) \\approx  \\text{Var}(a^{l+1}) $$ $$ \\text{Var}(\\delta^{l}) \\approx \\text{Var}(\\delta^{l+1}) $$ Tanh is linear around origin =&gt; \\(tanh(z) \\approx z\\) when z is small.</p> \\[\\begin{align*}  Var(a^{l+1}) &amp;= Var(z^{l+1}) \\\\  &amp; = Var(\\sum_{i=1}^{n_l} w_i^{l+1}a_i^l ) \\\\  &amp;= \\sum_{i=1}^{n_l} Var(w_i^{l+1}a_i^l )  \\\\  &amp;= \\sum_{i=1}^{n_l} Var(w_i^{l+1}) Var(a_i^l) \\\\   &amp;= n_{l} . Var(w_i^{l+1}) Var(a^l) \\\\  Var(W) &amp;= \\frac{1}{n_l}   \\end{align*}\\] <p>Similarly for gradient \\(Var(W) = \\frac{1}{n_{l+1}}\\)</p> <p>To balance the variance of the activations and gradients, we set: $ Var(W) = \\frac{2}{n_{in} + n_{out}} $</p>"},{"location":"Initializers/#kaiming-he-initialization","title":"Kaiming (He) Initialization","text":"<p>Paper Why Is It Needed? Xavier initialization assumes that the activation function is either linear, sigmoid, or tanh. However, ReLU is not linear(near origin). ReLU zeros out all negative values. On average, ReLU passes only about half of its inputs forward, effectively reducing the output variance by approximately 50%.  This reduction in variance can cause activations to shrink across layers, potentially leading to stalled training.  In such cases, Xavier initialization underestimates the necessary variance.</p> <p>Kaiming initialization is designed specifically for ReLU-like activations.</p> <p></p> <p>$$ W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}}) $$ The factor 2 compensates for the ReLU \u201cdrop\u201d of half the activations</p> <p>Goal of Xavier Initialization We want to keep the variance of the forward activations constant across layers. So: $$  \\text{Var}(a^l) \\approx  \\text{Var}(a^{l-1}) $$</p> \\[ \\begin{align*} Var(z^{l}) &amp;= Var(\\sum_{i=1}^{n_{in}} w_i^{l}a_i^{l-1} ) \\\\ &amp; = \\sum_{i=1}^{n_{in}} Var(w_i^{l}a_i^{l-1})  \\\\ &amp;= n_{in}  Var(w_i^{l}a_i^{l-1})  \\\\ &amp;= n_{in}(Var(w_i^{l}) Var(a_i^{l-1}) + \\mathbb{E}[w_i^l]^2Var(a_i^{l-1}) + \\mathbb{E}[a_i^{l-1}]^2 Var(w_i^l))\\\\ &amp;= n_{in} (Var(w_i^l) (Var(a_i)^{l-1} + E[a_i^{l-1}]^2)) \\\\ &amp;= n_{in} (Var(w_i^l) \\mathbb{E}[(a_i^{l-1})^2]) \\end{align*}\\] \\[ \\begin{align*} \\mathbb{E}[(z_i^{l-1})^2]) &amp;= \\int_{-\\infty}^{\\infty} (a_i^{l-1})^2 P_a(a_i^{l-1}) da_i^{l-1} \\\\ &amp;= \\int_{-\\infty}^{\\infty} max(0, z_i^{l-1})^2 P_a(z_i^{l-1}) dz_i^{l-1} \\\\ &amp;= \\int_{0}^{\\infty} (z_i^{l-1})^2 P_z(z_i^{l-1}) dz_i^{l-1}  \\\\ &amp;= \\frac{1}{2} \\int_{-\\infty}^{\\infty} (z_i^{l-1})^2 P_z(z_i^{l-1}) dz_i^{l-1} \\\\ &amp;= \\frac{1}{2} Var(z_i^{l-1})\\\\ \\end{align*}\\] <p>$$ Var(W) = \\frac{2}{n_{in}} $$ Empirically, the backward gradients will also behave reasonably as long as the forward activations are well-scaled. So unlike xavier we only try to maintain variance of activations.</p>"},{"location":"Initializers/#qa","title":"Q&amp;A","text":"<ol> <li> <p>Why is weight initialization important in neural networks?</p> <ul> <li>Weight initialization is important because it can significantly affect the convergence speed and performance of the model. Proper initialization helps to maintain a good flow of gradients during training, preventing issues like vanishing or exploding gradients.</li> </ul> </li> <li> <p>Can biases be initialized to zero? Why or why not?</p> <ul> <li>Yes, biases can be initialized to zero. Unlike weights, initializing biases to zero does not cause symmetry issues because biases are added to the output of neurons and do not affect the learning of different features. In fact, initializing biases to zero is a common practice.</li> </ul> </li> <li> <p>How does weight initialization affect vanishing or exploding gradients?</p> <ul> <li>Weight initialization affects vanishing or exploding gradients by influencing the scale of the activations and gradients as they propagate through the layers. If weights are initialized too small, the gradients can vanish, leading to slow convergence. If weights are initialized too large, the gradients can explode, causing instability during training.</li> </ul> </li> <li> <p>Diagnose an issue in training that could be due to bad initialization.</p> <ul> <li>If the model is not learning at all or learning very slowly, it could be due to bad initialization.    Visualizing the gradients during training can help identify this issue.</li> </ul> </li> <li> <p>If your deep model is not learning, what role might initialization play and how would you investigate it?</p> <ul> <li>If a deep model isn\u2019t learning, I would suspect initialization if:<ul> <li>The loss remains constant or decreases very slowly. </li> <li>Activations or gradients vanish or explode. </li> <li>All neurons appear to behave identically (symmetry).</li> </ul> </li> <li>Visualizing gradients and activations across layers.</li> </ul> </li> </ol>"},{"location":"batch_norm/","title":"Batch Norm","text":"<p>Paper</p> <p>A technique used in deep learning to improve training speed, stability, and performance by normalizing the inputs to each layer. It helps mitigate issues like vanishing/exploding gradients and allows for higher learning rates.</p> <p>Training a deep neural network is challenging due to issues such as:</p> <ul> <li>Internal covariate shift: The distribution of inputs to each layer changes during training as weights are updated.</li> <li>Vanishing/exploding gradients: Gradients can become too small or too large, making training difficult.</li> <li>Training instability: Due to these shifts and gradient issues, network may train slowly or diverge.</li> </ul> <p>BatchNorm addresses these by normalizing the input to each layer, so the distribution remains more stable during training.</p>"},{"location":"batch_norm/#forward-pass","title":"Forward pass","text":"<p>For each mini-batch, BatchNorm normalizes the actications. Let input to a layer be \\(x \\in \\mathbb{R}^{m \\times d}\\) where m is the batch size and d is the number of features. For each feature \\(x_j\\), compute:</p> <ol> <li>Compute the mean: \\(\\mu_j = \\frac{1}{m} \\sum_{i=1}^{m} x_{ij}\\)</li> <li>Compute the variance: \\(\\sigma_j^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_{ij} - \\mu_j)^2\\)</li> <li>Normalize the feature: \\(\\hat{x}_{j} = \\frac{x_{j} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}}\\)    where \\(\\epsilon\\) is a small constant to avoid division by zero.</li> <li>Scale and shift: \\(y_{ij} = \\gamma_j \\hat{x}_{ij} + \\beta_j \\\\     y_{j} = \\gamma \\hat{x}_j + \\beta\\)    where \\(\\gamma_j\\) and \\(\\beta_j\\) are learnable parameters for each feature.</li> </ol>"},{"location":"batch_norm/#backward-pass","title":"Backward pass","text":"<p>During backpropagation, gradients are computed for \\(\\gamma\\) and \\(\\beta\\) as well.  </p> \\[\\frac{\\partial L}{\\partial \\gamma} = \\sum_{i=1}^m \\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial \\gamma} = \\sum_{i=1}^m \\delta_i \\hat{x}_i  \\\\   \\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^m \\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial \\beta} = \\sum_{i=1}^m \\delta_i\\] \\[ \\begin{align*} \\frac{\\partial L}{\\partial x_i} &amp;= \\sum_{j=1}^m \\frac{\\partial L}{\\partial \\hat{x}_j} \\frac{\\partial \\hat{x}_j}{\\partial x_i} \\\\ \\frac{\\partial L}{\\partial \\hat{x}_j} &amp;= \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial \\hat{x}_j} = \\delta_j \\gamma \\\\ \\frac{\\partial \\hat{x}_j}{\\partial x_i} &amp;= \\frac{1}{\\sigma} \\frac{\\partial (x_j - \\mu)}{\\partial x_i} + (x_j - \\mu) \\frac{\\partial 1 / \\sigma }{\\partial x_i} \\quad \\text{Let }~ \\sigma = \\sqrt{\\sigma^2 + \\epsilon}  \\\\ \\frac{\\partial \\hat{x}_j}{\\partial x_i} &amp;= \\frac{1}{\\sigma} \\frac{\\partial (x_j - \\mu)}{\\partial x_i} - \\frac{x_j - \\mu}{2 \\sigma^3 } \\frac{\\partial \\sigma^2 }{\\partial x_i} \\\\ \\frac{\\partial \\sigma^2}{\\partial x_i} &amp;= \\sum_{j=1}^m \\frac{\\partial (x_j^2 + \\mu^2 - 2x_j\\mu )}{\\partial x_i} =  \\frac{2}{N} (x_i - \\mu) ) \\\\ \\frac{\\partial \\hat{x}_j}{\\partial x_i} &amp;= \\frac{1}{\\sigma} \\frac{\\partial (x_j - \\mu)}{\\partial x_i} - \\frac{(x_j - \\mu)(x_i - \\mu)}{N \\sigma^3 }  \\end{align*}\\] <p>For j = i</p> \\[ \\frac{\\partial \\hat{x}_i}{\\partial x_i} = \\frac{m-1}{m \\sigma} - \\frac{(x_i - \\mu)^2}{N \\sigma^3 } \\\\ \\frac{\\partial L}{\\partial x_i}= \\delta_i \\gamma (\\frac{m-1}{m \\sigma} - \\frac{(x_i - \\mu)^2}{N \\sigma^3 })  \\] <p>For j != i</p> \\[\\frac{\\partial \\hat{x}_i}{\\partial x_i} = \\frac{-1}{m\\sigma} - \\frac{(x_j - \\mu)(x_i - \\mu)}{N \\sigma^3 } \\\\ \\frac{\\partial L}{\\partial x_i}= \\sum_{i \\ne j} \\delta_j \\gamma (\\frac{-1}{m\\sigma} - \\frac{(x_j - \\mu)(x_i - \\mu)}{N \\sigma^3 }) \\] <p>Combining both cases, we get:</p> \\[\\frac{\\partial L}{\\partial x_i} = \\frac{\\gamma}{m \\sigma}(m\\delta_i - \\sum_j \\delta_j - \\hat{x}_i \\sum_j \\delta_j \\hat{x}_j)\\]"},{"location":"batch_norm/#advantages","title":"Advantages","text":"<ul> <li>Keeps input distributions stable during training.</li> <li>Allows higher learning rates.</li> <li>Acts as a regularizer (due to added noise from mini-batch stats).</li> </ul>"},{"location":"batch_norm/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>During inference, we use running averages of the mean and variance computed during training instead of batch statistics.</li> <li>Use after fully connected or convolutional layers, before activation functions.</li> <li>Initialization of \\(\\gamma\\) and \\(\\beta\\) is typically set to 1 and 0 respectively.</li> </ul>"},{"location":"batch_norm/#when-not-to-use","title":"When not to use","text":"<ul> <li>In small batch sizes, where batch statistics may not be reliable.</li> <li>In RNNs or LSTMs, where temporal dependencies are crucial.</li> </ul>"},{"location":"convolution/","title":"Convolution","text":"<p>Input: \\(\\mathbf{X} \\in \\mathbb{R}^{C_{in} \\times H_{in} \\times W_{in} }\\) Kernel: \\(W \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_H \\times K_W}\\) Output: \\(\\mathbf{Y} \\in \\mathbb{R}^{C_{out} \\times H_{out} \\times W_{out}}\\) \\(\\delta = \\frac{\\partial L}{\\partial Y} \\in \\mathbb{R}^{C_{out} \\times H_{out} \\times W_{out}}\\)</p> <p>\\(H_{out} = \\frac{H_{in} + 2P - K}{stride} + 1,~ W_{out} = \\frac{W_{in} + 2P - K}{stride} + 1\\)</p>"},{"location":"convolution/#forward-pass","title":"Forward pass","text":"<p>stride = 1, padding = 0</p> \\[Y_{c,i,j} = \\sum_{k=0}^{C_{in}-1} \\sum_{m=0}^{K_H-1} \\sum_{n=0}^{K_W-1} W_{c,k,m,n}.X_{k, i+m, j+n} + b_c\\] <p>Vectorized form:</p> \\[\\tilde{W} \\in \\mathbb{R}^{C_{out} \\times (C_{in}.K_H.K_W)} \\\\ \\text{Unfold Input: }\\tilde{X} \\in \\mathbb{R}^{(C_{in}.K_H.K_W) \\times (H_{out}.W_{out})} \\\\ Y = \\tilde{W} \\cdot \\tilde{X} + b \\\\ \\]"},{"location":"convolution/#backward-pass","title":"Backward pass","text":"<p>Gradients w.r.t Weights:</p> \\[\\frac{\\partial L}{\\partial W_{c,k,m,n}} = \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{c,i,j}} \\frac{\\partial Y_{c,i,j}}{\\partial W_{c,k,m,n}} \\\\ \\frac{\\partial Y_{c,i,j}}{\\partial W_{c,k,m,n}} = X_{k,i+m,j+n} \\\\ \\frac{\\partial L}{\\partial W_{c,k,m,n}} = \\sum_{i,j} \\delta_{c,i,j} X_{k,i+m,j+n}\\] \\[ \\frac{\\partial L}{\\partial \\tilde{W}} = \\delta \\tilde{X}^T \\] <p>Gradients w.r.t bias:</p> \\[ \\frac{\\partial L}{\\partial b_c} = \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{c,i,j}} \\frac{\\partial Y_{c,i,j}}{\\partial b_c} \\\\ \\frac{\\partial Y_{c,i,j}}{\\partial b_c} = 1 \\\\ \\frac{\\partial L}{\\partial b_c} = \\sum_{i,j} \\delta_{c,i,j} \\] <p>Gradients w.r.t input:</p> <p>$$\\frac{\\partial L}{\\partial X_{k,p,q}} = \\sum_{c=0}^{C_{out} - 1} \\sum_{i=0}^{H_{out} - 1} \\sum_{j=0}^{W_{out} - 1} \\frac{\\partial L}{\\partial Y_{c,i,j}} \\frac{\\partial Y_{c,i,j}}{\\partial X_{k,p,q}} $$ \\(Y_{c,i,j}\\) depends on \\(X_{k,p,q}\\) iff: \\(p=i+m,~ q=j+n\\) </p> \\[\\frac{\\partial Y_{c,i,j}}{\\partial X_{k,p,q}} = \\begin{cases} W_{c,k,p-i,q-j} &amp; \\text{if } 0 \\le p-i \\lt K_H \\text{ and } 0 \\le q-j \\lt K_W \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\\\ \\frac{\\partial L}{\\partial X_{k,p,q}} = \\sum_{c=0}^{C_{out} - 1} \\sum_{i=0}^{H_{out} - 1} \\sum_{j=0}^{W_{out} - 1} \\delta_{c,i,j} W_{c,k,p-i,q-j}\\] <p>This is equivalent to a full convolution of the gradient \\(\\delta\\) with the flipped kernel. \\\\(flip(W_{c,k,m,n}) = W_{c,k,K_H-1-m, K_W-1-n}\\)</p> \\[ \\frac{\\partial L}{\\partial \\tilde{X}} = \\tilde{W}^T . \\delta \\]"},{"location":"max_pool/","title":"Max Pooling","text":"<p>Max pooling is a down-sampling operation commonly used in convolutional neural networks (CNNs). It reduces the spatial dimensions of the input feature map while retaining the most important information. The operation works by sliding a window (or kernel) over the input feature map and taking the maximum value within that window.</p>"},{"location":"max_pool/#forward-pass","title":"Forward Pass","text":"<p>During the forward pass, max pooling takes the maximum value from each window of the input feature map. The window size and stride determine how much the window moves across the input feature map.</p> <p>Pooling window size: \\(k \\times k\\) Stride: \\(s\\)</p> \\[ Y_{i,j} = \\max_{(m,n) \\in R_{i,j} } X_{m,n} \\] <p>where \\(R_{i,j}\\) is the receptive field of size \\(k \\times k\\) starting at \\((i.s,~j.s)\\).</p>"},{"location":"max_pool/#backward-pass","title":"Backward Pass","text":"<p>During the backward pass, max pooling propagates the gradient only to the positions that were selected during the forward pass. The gradient is set to zero for all other positions.</p> <p>Let  - \\(\\nabla Y\\) be the gradient from next layer - \\(\\nabla X\\) to be backpropagated to previous(input) layer</p> \\[\\nabla X_{m,n} = \\begin{cases}  \\nabla Y_{i,j}, \\quad \\text{if } X_{m,n} = Y_{i,j} ~\\text{and}~ (m,n) \\in R_{i,j} \\\\  0, \\quad \\text{ otherwise}  \\end{cases}\\] <p>If multiple elements in a region are equal to the max, gradients are usually assigned arbitrarily or split.</p>"},{"location":"optimizer/","title":"Optimizers","text":""},{"location":"optimizer/#sgd","title":"SGD","text":""},{"location":"optimizer/#sgd-with-momentum","title":"SGD with momentum","text":""},{"location":"optimizer/#rmsprop","title":"RMSProp","text":""},{"location":"optimizer/#adam","title":"Adam","text":""},{"location":"regularization/","title":"Regularization","text":""},{"location":"regularization/#l1","title":"L1","text":""},{"location":"regularization/#qa","title":"Q&amp;A","text":""},{"location":"regularization/#l2","title":"L2","text":""},{"location":"regularization/#qa_1","title":"Q&amp;A","text":""},{"location":"regularization/#dropout","title":"Dropout","text":"<p>Paper </p> <p>Dropout is a regularization technique used to prevent overfitting in neural networks.  It works by randomly setting a fraction of the input units to zero during training, which helps to prevent the model from becoming too reliant on any one feature. During inference, dropout is turned off, and the full network is used.</p> <p>Why is Dropout needed? Dropout helps address overfitting, especially in deep networks where the model might memorize the training data instead of learning general patterns.</p> <p>Acts as an ensemble of subnetworks, improving robustness.</p> <p>Disadvantages: </p> <ul> <li>Slower convergence during training due to randomness.</li> <li>Incompatible with certain architectures like RNNS, LSTMs, CNNs.</li> <li>Can lead to underfitting if the dropout rate is too high.</li> <li>Can conflict with batch normalization, as it introduces noise that can affect the normalization process.</li> </ul>"},{"location":"regularization/#qa_2","title":"Q&amp;A","text":"<ol> <li> <p>What happens to the network during inference if Dropout was used during training?</p> <ul> <li>During inference, all neurons are active, and the outputs are typically scaled to account for the dropped units during training.</li> </ul> </li> <li> <p>How does Dropout work in practice?</p> <ul> <li>At each training step, each neuron's output is set to zero with probability p (dropout rate). The remaining outputs are scaled by 1 / (1 - p) to keep the expected sum of activations the same.</li> </ul> </li> <li> <p>What is the variance of the activations in each hidden layer when dropout is and is not applied? Draw a plot to show how this quantity evolves over time for both models.</p> <ul> <li>When dropout is applied, the variance of activations is reduced due to the random dropping of units. Without dropout, the variance tends to be higher as all units contribute to the output. </li> </ul> </li> <li> <p>What happens when dropout and weight decay are used at the same time? Are the results additive? Are there diminished returns (or worse)? Do they cancel each other out? </p> <ul> <li>When dropout and weight decay are used together, they can complement each other.     Dropout reduces overfitting by randomly dropping units, while weight decay penalizes large weights.     There can be diminished returns if both techniques are too aggressive, leading to underfitting. They do not cancel each other out; rather, they work together to improve generalization.</li> </ul> </li> <li> <p>What happens if we apply dropout to the individual weights of the weight matrix rather than the activations?</p> <ul> <li>This is called DropConnect. Zeroing weights directly can lead to more erratic training, unless handled with care. It is also computationally more expensive.</li> </ul> </li> </ol>"},{"location":"math/calculus/","title":"Calculus","text":""},{"location":"math/calculus/#derivatives","title":"Derivatives","text":"<p>A derivative is the rate of change in a function with respect to changes in its arguments.  Derivatives can tell us how rapidly a loss function would increase or decrease were we to increase or decrease each parameter by an infinitesimally small amount</p> \\[ f'(x) = \\lim_{h \\rightarrow 0 } \\frac{f(x + h) - f(x)}{h} \\]"},{"location":"math/calculus/#partial-derivatives","title":"Partial Derivatives","text":"<p>The partial derivative of a function with respect to one of its arguments is the derivative of that function with respect to that argument, holding all other arguments constant.</p> <p>Suppose that the input of function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is an n-dimensional vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T\\). The gradient of the function \\(f\\) with respect to \\(\\mathbf{x}\\) is a vector of \\(n\\) partial derivatives:</p> \\[\\nabla_n f(\\mathbf{x}) = [\\partial_{x_1}f(\\mathbf{x}), \\partial_{x_2}f(\\mathbf{x}), \\dots, \\partial_{x_n}f(\\mathbf{x})]^T \\]"},{"location":"math/calculus/#matrix-derivatives","title":"Matrix derivatives","text":"<ul> <li>$\\nabla_{\\mathbf{x}} A\\mathbf{x} = A^T $</li> <li>$\\nabla_{\\mathbf{x}} \\mathbf{x}^TA\\mathbf{x} = (A + A^T)\\mathbf{x} $</li> <li>\\(\\nabla_{\\mathbf{X}}||X||_F^2 = 2X\\)</li> </ul>"},{"location":"math/linear-algebra/","title":"Linear Algebra","text":""},{"location":"math/linear-algebra/#notations","title":"Notations","text":"<p>Scalar \\(x \\in \\mathbb{R}\\) Vector $ \\mathbf{x} \\in \\mathbb{R}^n$</p> \\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots\\\\ x_n \\end{bmatrix} \\] <p>Matrix $ \\mathbf{X} \\in \\mathbb{R}^{n \\times m} $</p> \\[\\mathbf{X} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1m} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nm} \\end{bmatrix}\\] <p>We will use \\(\\mathbf{X}\\) to denote input matrix that contains training samples and features.</p>"},{"location":"math/linear-algebra/#matrix-operations","title":"Matrix Operations","text":"<p>Hadamard product: The elementwise product of two matrices, denoted by \\(\\circ\\).</p>"},{"location":"math/linear-algebra/#linear-transformations","title":"Linear Transformations","text":""},{"location":"math/linear-algebra/#vector-spaces-and-subspaces","title":"Vector Spaces and Subspaces","text":""},{"location":"math/linear-algebra/#eigenvalues-and-eigenvectors","title":"Eigenvalues and Eigenvectors","text":"<p>An eigenvector for a matrix \\(A\\) is a nonzero vector \\(\\mathbf{x}\\) such that \\(A\\mathbf{x} = c\\mathbf{x}\\), where c is some constant. The constant \\(c\\) is called the eigenvalue corresponding to the eigenvector \\(\\mathbf{x}\\). The eigenvalue problem can be solved by finding the roots of the characteristic polynomial:</p> \\[\\text{det}(A - cI) = 0\\]"},{"location":"math/linear-algebra/#matrix-factorizations","title":"Matrix Factorizations","text":""},{"location":"math/linear-algebra/#norms-and-distance-metrics","title":"Norms and Distance Metrics","text":"<p>Vector Norms: The norm of a vector tells us how big it is. A norm is a function \\(||.||\\) that maps a vector to a scalar and satisfies the following three properties:</p> <ol> <li>Non-negativity: \\(||\\mathbf{x}|| \\geq 0\\) and \\(||\\mathbf{x}|| = 0\\) if and only if \\(\\mathbf{x} = 0\\).</li> <li>Homogeneity: \\(||\\alpha \\mathbf{x}|| = |\\alpha| ||\\mathbf{x}||\\) for any scalar \\(\\alpha\\).</li> <li>Triangle inequality: \\(||\\mathbf{x} + \\mathbf{y}|| \\leq ||\\mathbf{x}|| + ||\\mathbf{y}||\\) for any vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).</li> </ol> <p>\\(\\mathcal{l}_p \\text{ norm: }  ||\\mathbf{x}||_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p}\\)</p> <p>Matrix Norms : The Frobenius norm of a matrix \\(\\mathbf{X}\\) is defined as:</p> \\[||\\mathbf{X}||_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n |x_{ij}|^2} = \\sqrt{\\text{trace}(\\mathbf{X}^T \\mathbf{X})}\\]"},{"location":"math/linear-algebra/#projections-and-orthogonality","title":"Projections and Orthogonality","text":""},{"location":"math/probability/","title":"Probability","text":"<p>Sample Space: entire set of possibilities for an experiment.  Event: subset of the sample space. Probability Distribution: describes how probabilities are assigned to events.</p> \\[ Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] \\\\ Var(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\] <p>If X &amp; Y are independent random variables, then:</p> \\[ Var(X + Y) = Var(X) + Var(Y) \\\\ Var(XY) = Var(X)Var(Y) + E[X]^2Var(Y) + E[Y]^2Var(X) \\]"},{"location":"math/probability/#discrete-distributions","title":"Discrete distributions","text":""},{"location":"math/probability/#continuous-distributions","title":"Continuous distributions","text":""},{"location":"math/probability/#central-limit-theorem","title":"Central Limit Theorem","text":"<p>Given a sufficiently large sample size, the sampling distribution of the sample mean will be approximately normally distributed, regardless of the shape of the population distribution, provided the population has a finite mean and variance.</p>"}]}