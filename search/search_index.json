{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Title"},{"location":"Initializers/","title":"Weight Initializers","text":""},{"location":"Initializers/#summary","title":"Summary","text":"Initialization Recommended For Preserves Variance Activation Function <code>Zero</code> bias - <code>Random</code> Small NN All <code>Xavier</code> Shallow / Mid-depth Tanh, Sigmoid <code>Kaiming</code> Deep NN ReLU, Leaky ReLU <p>Behaviour of diferent initializers with respect to the variance of activations and gradients can be seen here</p>"},{"location":"Initializers/#constant-initialization","title":"Constant Initialization","text":"<p>Weights of a layer are initialized to a fixed constant value.  Initializing all weights in a layer to a fixed constant value is not recommended.  When this happens, each neuron in the layer receives the same gradient during backpropagation, causing them to learn identical features.  This leads to redundancy, as the neurons fail to capture diverse representations.  Consequently, the model\u2019s ability to learn complex patterns is significantly reduced.</p> \\[ \\mathbf{W}_{ij} = c \\quad \\forall ~ i,j \\]"},{"location":"Initializers/#random-initialization","title":"Random Initialization","text":"<p>Weights of a layer are initialized to random values.  Unlike constant initialization, this randomness enables each neuron to learn distinct features. The weights are usually drawn from a uniform or normal distribution, such as: $$ \\mathbf{W}_{ij} \\sim \\mathcal{N}(\\mu, \\sigma^2) $$ While this approach works reasonably well for shallow networks, it can lead to issues like vanishing or exploding gradients in deeper architectures. This occurs because the variance of activations and gradients may increase or decrease exponentially with the number of layers, primarily due to not accounting for the number of input and output neurons in the initialization scheme.</p>"},{"location":"Initializers/#xavier-glorot-initialization","title":"Xavier (Glorot) Initialization","text":"<p>Paper Xavier initialization aims to keep the variance of the activations and gradients constant across layers.  </p> <p>Why Is It Needed? If weights are not initialized properly: </p> <ul> <li>Too small: Activations and gradients shrink \u21d2 vanishing gradients </li> <li>Too large: Activations and gradients blow up \u21d2 exploding gradients  </li> </ul> <p></p> <p>Xavier initialization solves this by scaling weights based on the number of input and output units in the layer. It assumes the activation functions are linear, sigmoid, or tanh. Let:</p> <ul> <li>\\(n_{in}\\) : number of input units to a layer</li> <li>\\(n_{out}\\) : number of output units from a layer</li> </ul> \\[ W \\sim \\mathcal{N}(0, \\frac{2}{n_{in} + n_{out}}) \\] <p>Goal of Xavier Initialization Assume weights \\(w_i\\) and \\(x_i\\) are independent and identically distributed (i.i.d.) random variables.  </p> <p>We want to keep the variance of the activations and gradients constant across layers. So: $$  \\text{Var}(a^l) \\approx  \\text{Var}(a^{l+1}) $$ $$ \\text{Var}(\\delta^{l}) \\approx \\text{Var}(\\delta^{l+1}) $$ Tanh is linear around origin =&gt; \\(tanh(z) \\approx z\\) when z is small.</p> \\[\\begin{align*}  Var(a^{l+1}) &amp;= Var(z^{l+1}) \\\\  &amp; = Var(\\sum_{i=1}^{n_l} w_i^{l+1}a_i^l ) \\\\  &amp;= \\sum_{i=1}^{n_l} Var(w_i^{l+1}a_i^l )  \\\\  &amp;= \\sum_{i=1}^{n_l} Var(w_i^{l+1}) Var(a_i^l) \\\\   &amp;= n_{l} . Var(w_i^{l+1}) Var(a^l) \\\\  Var(W) &amp;= \\frac{1}{n_l}   \\end{align*}\\] <p>Similarly for gradient \\(Var(W) = \\frac{1}{n_{l+1}}\\)</p> <p>To balance the variance of the activations and gradients, we set: $ Var(W) = \\frac{2}{n_{in} + n_{out}} $</p>"},{"location":"Initializers/#kaiming-he-initialization","title":"Kaiming (He) Initialization","text":"<p>Paper Why Is It Needed? Xavier initialization assumes that the activation function is either linear, sigmoid, or tanh. However, ReLU is not linear(near origin). ReLU zeros out all negative values. On average, ReLU passes only about half of its inputs forward, effectively reducing the output variance by approximately 50%.  This reduction in variance can cause activations to shrink across layers, potentially leading to stalled training.  In such cases, Xavier initialization underestimates the necessary variance.</p> <p>Kaiming initialization is designed specifically for ReLU-like activations.</p> <p></p> <p>$$ W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}}) $$ The factor 2 compensates for the ReLU \u201cdrop\u201d of half the activations</p> <p>Goal of Xavier Initialization We want to keep the variance of the forward activations constant across layers. So: $$  \\text{Var}(a^l) \\approx  \\text{Var}(a^{l-1}) $$</p> \\[ \\begin{align*} Var(z^{l}) &amp;= Var(\\sum_{i=1}^{n_{in}} w_i^{l}a_i^{l-1} ) \\\\ &amp; = \\sum_{i=1}^{n_{in}} Var(w_i^{l}a_i^{l-1})  \\\\ &amp;= n_{in}  Var(w_i^{l}a_i^{l-1})  \\\\ &amp;= n_{in}(Var(w_i^{l}) Var(a_i^{l-1}) + \\mathbb{E}[w_i^l]^2Var(a_i^{l-1}) + \\mathbb{E}[a_i^{l-1}]^2 Var(w_i^l))\\\\ &amp;= n_{in} (Var(w_i^l) (Var(a_i)^{l-1} + E[a_i^{l-1}]^2)) \\\\ &amp;= n_{in} (Var(w_i^l) \\mathbb{E}[(a_i^{l-1})^2]) \\end{align*}\\] \\[ \\begin{align*} \\mathbb{E}[(z_i^{l-1})^2]) &amp;= \\int_{-\\infty}^{\\infty} (a_i^{l-1})^2 P_a(a_i^{l-1}) da_i^{l-1} \\\\ &amp;= \\int_{-\\infty}^{\\infty} max(0, z_i^{l-1})^2 P_a(z_i^{l-1}) dz_i^{l-1} \\\\ &amp;= \\int_{0}^{\\infty} (z_i^{l-1})^2 P_z(z_i^{l-1}) dz_i^{l-1}  \\\\ &amp;= \\frac{1}{2} \\int_{-\\infty}^{\\infty} (z_i^{l-1})^2 P_z(z_i^{l-1}) dz_i^{l-1} \\\\ &amp;= \\frac{1}{2} Var(z_i^{l-1})\\\\ \\end{align*}\\] <p>$$ Var(W) = \\frac{2}{n_{in}} $$ Empirically, the backward gradients will also behave reasonably as long as the forward activations are well-scaled. So unlike xavier we only try to maintain variance of activations.</p>"},{"location":"Initializers/#qa","title":"Q&amp;A","text":"<ol> <li> <p>Why is weight initialization important in neural networks?</p> <ul> <li>Weight initialization is important because it can significantly affect the convergence speed and performance of the model. Proper initialization helps to maintain a good flow of gradients during training, preventing issues like vanishing or exploding gradients.</li> </ul> </li> <li> <p>Can biases be initialized to zero? Why or why not?</p> <ul> <li>Yes, biases can be initialized to zero. Unlike weights, initializing biases to zero does not cause symmetry issues because biases are added to the output of neurons and do not affect the learning of different features. In fact, initializing biases to zero is a common practice.</li> </ul> </li> <li> <p>How does weight initialization affect vanishing or exploding gradients?</p> <ul> <li>Weight initialization affects vanishing or exploding gradients by influencing the scale of the activations and gradients as they propagate through the layers. If weights are initialized too small, the gradients can vanish, leading to slow convergence. If weights are initialized too large, the gradients can explode, causing instability during training.</li> </ul> </li> <li> <p>Diagnose an issue in training that could be due to bad initialization.</p> <ul> <li>If the model is not learning at all or learning very slowly, it could be due to bad initialization.    Visualizing the gradients during training can help identify this issue.</li> </ul> </li> <li> <p>If your deep model is not learning, what role might initialization play and how would you investigate it?</p> <ul> <li>If a deep model isn\u2019t learning, I would suspect initialization if:<ul> <li>The loss remains constant or decreases very slowly. </li> <li>Activations or gradients vanish or explode. </li> <li>All neurons appear to behave identically (symmetry).</li> </ul> </li> <li>Visualizing gradients and activations across layers.</li> </ul> </li> </ol>"},{"location":"batch_norm/","title":"Batch Norm","text":"<p>Paper</p>"},{"location":"convolution/","title":"Convolution","text":"<p>Input: \\(\\mathbf{X} \\in \\mathbb{R}^{C_{in} \\times H_{in} \\times W_{in} }\\) Kernel: \\(W \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_H \\times K_W}\\) Output: \\(\\mathbf{Y} \\in \\mathbb{R}^{C_{out} \\times H_{out} \\times W_{out}}\\) \\(\\delta = \\frac{\\partial L}{\\partial Y} \\in \\mathbb{R}^{C_{out} \\times H_{out} \\times W_{out}}\\)</p> <p>\\(H_{out} = \\frac{H_{in} + 2P - K}{stride} + 1,~ W_{out} = \\frac{W_{in} + 2P - K}{stride} + 1\\)</p>"},{"location":"convolution/#forward-pass","title":"Forward pass","text":"<p>stride = 1, padding = 0</p> \\[Y_{c,i,j} = \\sum_{k=0}^{C_{in}-1} \\sum_{m=0}^{K_H-1} \\sum_{n=0}^{K_W-1} W_{c,k,m,n}.X_{k, i+m, j+n} + b_c\\] <p>Vectorized form:</p> \\[\\tilde{W} \\in \\mathbb{R}^{C_{out} \\times (C_{in}.K_H.K_W)} \\\\ \\text{Unfold Input: }\\tilde{X} \\in \\mathbb{R}^{(C_{in}.K_H.K_W) \\times (H_{out}.W_{out})} \\\\ Y = \\tilde{W} \\cdot \\tilde{X} + b \\\\ \\]"},{"location":"convolution/#backward-pass","title":"Backward pass","text":"<p>Gradients w.r.t Weights:</p> \\[\\frac{\\partial L}{\\partial W_{c,k,m,n}} = \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{c,i,j}} \\frac{\\partial Y_{c,i,j}}{\\partial W_{c,k,m,n}} \\\\ \\frac{\\partial Y_{c,i,j}}{\\partial W_{c,k,m,n}} = X_{k,i+m,j+n} \\\\ \\frac{\\partial L}{\\partial W_{c,k,m,n}} = \\sum_{i,j} \\delta_{c,i,j} X_{k,i+m,j+n}\\] \\[ \\frac{\\partial L}{\\partial \\tilde{W}} = \\delta \\tilde{X}^T \\] <p>Gradients w.r.t bias:</p> \\[ \\frac{\\partial L}{\\partial b_c} = \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{c,i,j}} \\frac{\\partial Y_{c,i,j}}{\\partial b_c} \\\\ \\frac{\\partial Y_{c,i,j}}{\\partial b_c} = 1 \\\\ \\frac{\\partial L}{\\partial b_c} = \\sum_{i,j} \\delta_{c,i,j} \\] <p>Gradients w.r.t input:</p> <p>$$\\frac{\\partial L}{\\partial X_{k,p,q}} = \\sum_{c=0}^{C_{out} - 1} \\sum_{i=0}^{H_{out} - 1} \\sum_{j=0}^{W_{out} - 1} \\frac{\\partial L}{\\partial Y_{c,i,j}} \\frac{\\partial Y_{c,i,j}}{\\partial X_{k,p,q}} $$ \\(Y_{c,i,j}\\) depends on \\(X_{k,p,q}\\) iff: \\(p=i+m,~ q=j+n\\) </p> \\[\\frac{\\partial Y_{c,i,j}}{\\partial X_{k,p,q}} = \\begin{cases} W_{c,k,p-i,q-j} &amp; \\text{if } 0 \\le p-i \\lt K_H \\text{ and } 0 \\le q-j \\lt K_W \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\\\ \\frac{\\partial L}{\\partial X_{k,p,q}} = \\sum_{c=0}^{C_{out} - 1} \\sum_{i=0}^{H_{out} - 1} \\sum_{j=0}^{W_{out} - 1} \\delta_{c,i,j} W_{c,k,p-i,q-j}\\] <p>This is equivalent to a full convolution of the gradient \\(\\delta\\) with the flipped kernel. \\\\(flip(W_{c,k,m,n}) = W_{c,k,K_H-1-m, K_W-1-n}\\)</p> \\[ \\frac{\\partial L}{\\partial \\tilde{X}} = \\tilde{W}^T . \\delta \\]"},{"location":"max_pool/","title":"Max Pooling","text":"<p>Max pooling is a down-sampling operation commonly used in convolutional neural networks (CNNs). It reduces the spatial dimensions of the input feature map while retaining the most important information. The operation works by sliding a window (or kernel) over the input feature map and taking the maximum value within that window.</p>"},{"location":"max_pool/#forward-pass","title":"Forward Pass","text":"<p>During the forward pass, max pooling takes the maximum value from each window of the input feature map. The window size and stride determine how much the window moves across the input feature map.</p> <p>Pooling window size: \\(k \\times k\\) Stride: \\(s\\)</p> \\[ Y_{i,j} = \\max_{(m,n) \\in R_{i,j} } X_{m,n} \\] <p>where \\(R_{i,j}\\) is the receptive field of size \\(k \\times k\\) starting at \\((i.s,~j.s)\\).</p>"},{"location":"max_pool/#backward-pass","title":"Backward Pass","text":"<p>During the backward pass, max pooling propagates the gradient only to the positions that were selected during the forward pass. The gradient is set to zero for all other positions.</p> <p>Let  - \\(\\nabla Y\\) be the gradient from next layer - \\(\\nabla X\\) to be backpropagated to previous(input) layer</p> \\[\\nabla X_{m,n} = \\begin{cases}  \\nabla Y_{i,j}, \\quad \\text{if } X_{m,n} = Y_{i,j} ~\\text{and}~ (m,n) \\in R_{i,j} \\\\  0, \\quad \\text{ otherwise}  \\end{cases}\\] <p>If multiple elements in a region are equal to the max, gradients are usually assigned arbitrarily or split.</p>"},{"location":"optimizer/","title":"Optimizers","text":""},{"location":"optimizer/#sgd","title":"SGD","text":""},{"location":"optimizer/#sgd-with-momentum","title":"SGD with momentum","text":""},{"location":"optimizer/#rmsprop","title":"RMSProp","text":""},{"location":"optimizer/#adam","title":"Adam","text":""},{"location":"math/calculus/","title":"Calculus","text":""},{"location":"math/calculus/#derivatives","title":"Derivatives","text":"<p>A derivative is the rate of change in a function with respect to changes in its arguments.  Derivatives can tell us how rapidly a loss function would increase or decrease were we to increase or decrease each parameter by an infinitesimally small amount</p> \\[ f'(x) = \\lim_{h \\rightarrow 0 } \\frac{f(x + h) - f(x)}{h} \\]"},{"location":"math/calculus/#partial-derivatives","title":"Partial Derivatives","text":"<p>The partial derivative of a function with respect to one of its arguments is the derivative of that function with respect to that argument, holding all other arguments constant.</p> <p>Suppose that the input of function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is an n-dimensional vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T\\). The gradient of the function \\(f\\) with respect to \\(\\mathbf{x}\\) is a vector of \\(n\\) partial derivatives:</p> \\[\\nabla_n f(\\mathbf{x}) = [\\partial_{x_1}f(\\mathbf{x}), \\partial_{x_2}f(\\mathbf{x}), \\dots, \\partial_{x_n}f(\\mathbf{x})]^T \\]"},{"location":"math/calculus/#matrix-derivatives","title":"Matrix derivatives","text":"<ul> <li>$\\nabla_{\\mathbf{x}} A\\mathbf{x} = A^T $</li> <li>$\\nabla_{\\mathbf{x}} \\mathbf{x}^TA\\mathbf{x} = (A + A^T)\\mathbf{x} $</li> <li>\\(\\nabla_{\\mathbf{X}}||X||_F^2 = 2X\\)</li> </ul>"},{"location":"math/linear-algebra/","title":"Linear Algebra","text":""},{"location":"math/linear-algebra/#notations","title":"Notations","text":"<p>Scalar \\(x \\in \\mathbb{R}\\) Vector $ \\mathbf{x} \\in \\mathbb{R}^n$</p> \\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots\\\\ x_n \\end{bmatrix} \\] <p>Matrix $ \\mathbf{X} \\in \\mathbb{R}^{n \\times m} $</p> \\[\\mathbf{X} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1m} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nm} \\end{bmatrix}\\] <p>We will use \\(\\mathbf{X}\\) to denote input matrix that contains training samples and features.</p>"},{"location":"math/linear-algebra/#matrix-operations","title":"Matrix Operations","text":"<p>Hadamard product: The elementwise product of two matrices, denoted by \\(\\circ\\).</p>"},{"location":"math/linear-algebra/#linear-transformations","title":"Linear Transformations","text":""},{"location":"math/linear-algebra/#vector-spaces-and-subspaces","title":"Vector Spaces and Subspaces","text":""},{"location":"math/linear-algebra/#eigenvalues-and-eigenvectors","title":"Eigenvalues and Eigenvectors","text":"<p>An eigenvector for a matrix \\(A\\) is a nonzero vector \\(\\mathbf{x}\\) such that \\(A\\mathbf{x} = c\\mathbf{x}\\), where c is some constant. The constant \\(c\\) is called the eigenvalue corresponding to the eigenvector \\(\\mathbf{x}\\). The eigenvalue problem can be solved by finding the roots of the characteristic polynomial:</p> \\[\\text{det}(A - cI) = 0\\]"},{"location":"math/linear-algebra/#matrix-factorizations","title":"Matrix Factorizations","text":""},{"location":"math/linear-algebra/#norms-and-distance-metrics","title":"Norms and Distance Metrics","text":"<p>Vector Norms: The norm of a vector tells us how big it is. A norm is a function \\(||.||\\) that maps a vector to a scalar and satisfies the following three properties:</p> <ol> <li>Non-negativity: \\(||\\mathbf{x}|| \\geq 0\\) and \\(||\\mathbf{x}|| = 0\\) if and only if \\(\\mathbf{x} = 0\\).</li> <li>Homogeneity: \\(||\\alpha \\mathbf{x}|| = |\\alpha| ||\\mathbf{x}||\\) for any scalar \\(\\alpha\\).</li> <li>Triangle inequality: \\(||\\mathbf{x} + \\mathbf{y}|| \\leq ||\\mathbf{x}|| + ||\\mathbf{y}||\\) for any vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).</li> </ol> <p>\\(\\mathcal{l}_p \\text{ norm: }  ||\\mathbf{x}||_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p}\\)</p> <p>Matrix Norms : The Frobenius norm of a matrix \\(\\mathbf{X}\\) is defined as:</p> \\[||\\mathbf{X}||_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n |x_{ij}|^2} = \\sqrt{\\text{trace}(\\mathbf{X}^T \\mathbf{X})}\\]"},{"location":"math/linear-algebra/#projections-and-orthogonality","title":"Projections and Orthogonality","text":""},{"location":"math/probability/","title":"Probability","text":"<p>Sample Space: entire set of possibilities for an experiment.  Event: subset of the sample space. Probability Distribution: describes how probabilities are assigned to events.</p> \\[ Var(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] $$ $$ Var(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\] <p>If X &amp; Y are independent random variables, then:</p> \\[ Var(X + Y) = Var(X) + Var(Y) \\\\ Var(XY) = Var(X)Var(Y) + E[X]^2Var(Y) + E[Y]^2Var(X) \\]"},{"location":"math/probability/#discrete-distributions","title":"Discrete distributions","text":""},{"location":"math/probability/#continuous-distributions","title":"Continuous distributions","text":""},{"location":"math/probability/#central-limit-theorem","title":"Central Limit Theorem","text":"<p>Given a sufficiently large sample size, the sampling distribution of the sample mean will be approximately normally distributed, regardless of the shape of the population distribution, provided the population has a finite mean and variance.</p>"}]}